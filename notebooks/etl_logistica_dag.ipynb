{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD_1p8ok6ZnE"
      },
      "outputs": [],
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def run_etl():\n",
        "\n",
        "    raw = pd.read_csv('/opt/airflow/dags/data/raw/deliveries_sample.csv')\n",
        "    raw['delivery_time_days'] = (pd.to_datetime(raw['delivered_at']) - pd.to_datetime(raw['order_date'])).dt.days\n",
        "    processed = raw.dropna()\n",
        "    processed.to_parquet('/opt/airflow/dags/data/processed/cleaned_deliveries.parquet')\n",
        "    print(\"ETL concluído!\")\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'sidney',\n",
        "    'start_date': datetime(2025, 1, 1),\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    'etl_logistica_dag',\n",
        "    default_args=default_args,\n",
        "    description='Pipeline de ETL de logística',\n",
        "    schedule_interval='0 6 * * *',  # roda todos os dias às 6h\n",
        "    catchup=False,\n",
        ") as dag:\n",
        "\n",
        "    etl_task = PythonOperator(\n",
        "        task_id='executar_etl',\n",
        "        python_callable=run_etl,\n",
        "    )\n",
        "\n",
        "etl_task\n"
      ]
    }
  ]
}